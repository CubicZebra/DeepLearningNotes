#$L^1$参数正则化与特征选择

与$L^{2}$正则化相类似，不考虑偏置的情形，我们将$L^{1}$正则化项写成一个自定义超参数$\alpha$与权重$\boldsymbol{\omega}$的一范数相乘的一项，则有：

$$\widetilde{J}(\boldsymbol{\omega; X, y})= J(\boldsymbol{\omega; X, y}) + \alpha\Vert\boldsymbol{\omega}\Vert_1 = J(\boldsymbol{\omega; X, y}) + \sum_{i}\alpha|\omega_i| \tag{1}$$

为求近似，我们保留上式中的第二项，将上式中的前一项在$\boldsymbol{\omega^*}$处进行泰勒展开，根跟定义可知$\boldsymbol{J}$在$\boldsymbol{\omega^*}$处的Jacobian为0，则有：

$$\widetilde{J}(\boldsymbol{\omega; X, y})= J(\boldsymbol{\omega^*; X, y}) + \frac{1}{2}(\boldsymbol{\omega}-\boldsymbol{\omega}^*)^\intercal \boldsymbol{H} (\boldsymbol{\omega}-\boldsymbol{\omega}^*) + \sum_{i}\alpha|\omega_i| \tag{2}$$

可以在局部小的区域作二次近似（严格的凸函数），不妨假设上式在$\boldsymbol{\omega^*}$处取得唯一最小值，那么$\boldsymbol{H}$一定为正定矩阵，满秩且可逆，因此我们可以通过数据预处理将$\boldsymbol{X}$变换为合适的$\boldsymbol{X'}$后，实现对原Hessian的对角化：

$$\widetilde{J}(\boldsymbol{\omega; X', y})= J(\boldsymbol{\omega^*; X', y}) + \frac{1}{2}(\boldsymbol{\omega}-\boldsymbol{\omega}^*)^\intercal \boldsymbol{\Lambda} (\boldsymbol{\omega}-\boldsymbol{\omega}^*) + \sum_{i}\alpha|\omega_i| \tag{3}$$

因为严格凸性的前提，可知$\Lambda$中所有元素均大于0，若定义$det|\Lambda| = \sum_{i}H_{i,i}$，就可以把式(3)还原成书中式(7.22)的形式：

$$\widehat{J}(\boldsymbol{\omega; X', y})= J(\boldsymbol{\omega^*; X', y}) + \sum_{i}[\frac{1}{2}H_{i,i}(\omega_i-\omega_i^{*})^2+\alpha|\omega_i|] \tag{4}$$

$\alpha|\omega_i|$的一阶偏导为符号函数，在0点不可导，除0点外，我们对式(4)向$\omega_i$求二阶偏导可知其结果正好为$H_{i,i} > 0$，经处理后的Hessian阵为对角矩阵，意味着所有的$\omega_i$均线性无关，也就是说，分别调整每一维的$\omega_i$使$\omega_i = argmin_{\omega_i}J(\boldsymbol{\omega; X', y})$后（一阶偏导数为0），$\boldsymbol{J}$也就取得了全局最小值。

式(4)对$\omega_i$求一阶偏导数后有：

$$\frac{\partial \widehat{J}(\boldsymbol{\omega; X', y})}{\partial \omega_i}= H_{i,i}(\omega_i-\omega_i^{*})+\alpha \cdot sign(\omega_i) \tag{5}$$

使上式为0，有：

$$\omega_i = \omega_i^*-\frac{\alpha}{H_{i,i}}sign(\omega_i) = |\omega_i^*|\cdot sign(\omega_i^*)-\frac{\alpha}{H_{i,i}}sign(\omega_i) \tag{6}$$

我们前面已做过假设使其在$\boldsymbol{\omega^*}$处取得唯一最小值，观察式(4)，在$|\omega_i|$不变的情况下，为使其取得最小值，$\omega_i$应与$\omega_i^*$同号，即$sign(\omega_i) = sign(\omega_i^*)$，将其代入式(6)中有：

$$\omega_i = |\omega_i^*|\cdot sign(\omega_i^*)-\frac{\alpha}{H_{i,i}}sign(\omega_i^*) = sign(\omega_i^*)(|\omega_i^*|-\frac{\alpha}{H_{i,i}}) \tag{7}$$

通过上式我们就可以明显看出$L^{1}$正则化与$L^{2}$相比的最大不同：对于给定的超参数$\alpha$，其中任意一个维度i的参数$\omega_i$的$L^{1}$方法是从$\omega_i^*$向0的方向行进$(\alpha/H_{i,i})$的步长，但同时还必须满足$sign(\omega_i) = sign(\omega_i^*)$的约束，因此当$|\omega_i^*|\leq|\frac{\alpha}{H_{i,i}}|$时，$\omega_i$只能取到0，因此其最优解也就写成了书中式(7.23)的形式。

通过增大$\alpha$的值，$L^{1}$方法可以使更多的$\omega_i$变为0，以此获得更为稀疏的解，这是其与$L^{2}$方法相比最本质的不同。这种特性使其广泛地被用于机器学习中的特征选择，从而达到简化机器学习的问题的目的。
