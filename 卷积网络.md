##第九章 卷积网络

###9.1 卷积运算

*P.282*

使式(9.4)中的$i - m = m'$以及$j - n = n'$代回原式即可得到“翻转”后的式(9.5)。同样，如果把$m' = -m''$以及$n' = -n''$代回式(9.5)将会得到：

$$S(i,j) = (K \ast I)(i,j)=\sum_{m’’} \sum_{n''} I(i+m'',j+n'')K(-m'',-n'')$$

这同样也是由式(9.4)经“翻转”而来，核与输入的索引之间此消彼长的性质依旧不变。若不对核进行翻转，我们用$m = -m''$以及$n = -n''$仅代回上式的核函数K，并将输入里的$m''$和$n''$都写为$m$和$n$的形式就得到了不具备翻转性质的式(9.6)，可看到输入与核函数的索引同增同减的性质。

###9.2 动机

*P.284*

在稀疏连接中，假设输入和输出都为n，且不存在激活函数的情况下，假设对于任意一个输出，其接受域为3，那么对于其权重矩阵$\boldsymbol{W} \in \mathbb{R}^{n \times n}$，$\forall |i-j| > 2, \quad w_{ij} = 0$，$\boldsymbol{W}$为三对角矩阵，非零元素大量集中在对角线附近，其排列是稀疏的。

*P.285*

等变性质：设$I(i,j)$经平移$(a,b)$后变为$I(i'-a,j'-b)$，所以$I(i'-a,j'-b) = I(i,j)$，根据式(9.6)：

$$S(i'-a,j'-b) = \sum_{m} \sum_{n} I(i'-a+m,j'-b+n)K(m,n) = \sum_{m} \sum_{n} I(i+m,j+n)K(m,n) = S(i,j)$$

$S(i'-a,j'-b) = S(i,j)$，所以原图先经平移再变换后的结果，和原图直接变换后的结果，仍具有平移性质。

*P.287*

把核函数看成一个filter，参数共享决定了我们不用针对输入图片像素的幂次级别的参数数进行逐个学习，而是可以仅靠习得filter内有限像素的合适参数，获得一个满足任务要求的核。如果我们用的是同一个核，在输入的不同位置如果参数不一样，那么该核对模式的识别能力也将因位置而发生变化，这是机器学习任务所不希望看到的。参数共享能很好地解决这个问题。以图9.5的上图为例，参数共享决定了$s_2$对$x_1$、$x_2$及$x_3$的提取方式与$s_3$对$x_2$、$x_3$及$x_4$的提取方式一致，不因位置的关系而产生差异(如$s_2 = a \cdot x_1 + b \cdot x_2 + c \cdot x_3$则一定$s_3 = a \cdot x_2 + b \cdot x_3 + c \cdot x_4$)。如此看来无论输入有多大，卷积层能够实现”在有限接受域内“提取特征的功能的同时，又保持了这种特性的平移等变性。

###9.5 基本卷积函数的变体

*P.296*

带”步幅“(stride)的卷积计算公式有误，应为：

$$Z_{i,j,k} = c(\boldsymbol{K},\boldsymbol{V},s)_{i,j,k} = \sum_{l,m,n}[V_{l,js+m-1,ks+n-1}K_{i,l,m,n}]$$

*P.300*

标准卷积是对一种模式的提取。在图像中，我们几乎可以认为在数个像素点的变化中图像是基本不变的。可以看出，平铺卷积的意义在于，在针对局部的“基本不变”的输入，仅使用一次卷积的代价，同时对多种特征进行提取。

*P.301*

带”步幅“(stride)的反向传播公式有误，注意i为输入的层索引，j为输出的层索引，k和l对应了输入中的绝对位置(宽和高的像素坐标)，式(9.11)对应着式(9.8)应写为：

$$
\begin{align}
g(\boldsymbol{G},\boldsymbol{V},s)_{i,l,m,n}
&= \frac{\partial}{\partial K_{i,l,m,n}}J(\boldsymbol{V},\boldsymbol{K}) = \sum_{j,k} \frac{\partial J(\boldsymbol{V},\boldsymbol{K})}{\partial Z_{i,j,k}} \frac{\partial Z_{i,j,k}}{\partial K_{i,l,m,n}} \\
&= \sum_{j,k}[G_{i,j,k}V_{l,js+m-1,ks+n-1}]
\end{align}
$$

这个结果是对应着代价函数对卷积层权重的梯度，更一般地，我们希望得到代价函数对被卷积层的梯度，则有：

$$\frac{\partial}{\partial V_{l,a,b}}J(\boldsymbol{V},\boldsymbol{K}) = \sum_{i,j,k} \frac{\partial J(\boldsymbol{V},\boldsymbol{K})}{\partial Z_{i,j,k}} \frac{\partial Z_{i,j,k}}{\partial V_{l,a,b}}
= \sum_{i,j,k}[G_{i,j,k} \sum_{m\; s.t.\\m=a+1-js} \sum_{n\; s.t.\\n=b+1-ks} K_{i,l,m,n}]$$

上式的各下标与之前的推导保持一致，而且比式(9.13)容易理解得多。按定义式来看该运算涉及大量递归的线性运算的过程。

虽然卷积运算定义式支持完整的反向传播运算，但通过上式也看出，对完整的$V_{l,a,b}$求和后涉及到对两张量的共计八次求和操作，其运算代价相当大，很难适用大规模的训练模型。如9.9节中提到，卷积网络常用来无监督地提取样本特征，然后再用相对低的代价进行训练。