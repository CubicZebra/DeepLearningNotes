#二阶近似方法

##共轭梯度法

二阶近似方法主要对应着非线性最优化的知识点。原书中有一处表述不当：式(8.29)中的$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})$前应有负号(因为$\boldsymbol{d}_t$更新的方向应靠近负梯度的方向)。

之所以提到二阶近似的方法，需要明确它与一阶方法最主要的两点区别：一、在一阶方法如AdaGrad，RMSProp和Adam，学习率的选取往往是主观的、经验性的；二、一阶方法的迭代次数是没有保证的，往往设计成一个比较大的值，而二阶方法无论是共轭梯度，还是拟牛顿法，都具备在有限步骤(对探索的向量维度相同的量级，如最优化一个N维严格凸函数，共轭梯度所需要的迭代步数法理论上不超过N)达到最优解的特性。

在线性共轭梯度法中，若$\boldsymbol{x}^{t+1} = \boldsymbol{x}^{t} + \alpha_t \boldsymbol{d}_t$中的步长因子$\alpha_t = (\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}))^\intercal \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}) /[{(\boldsymbol{d}^t)^\intercal \boldsymbol{A} \boldsymbol{d}^t}]$很难用显示表示出来，则需要用精确(每步都需要求$(\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}))$与$\boldsymbol{d}^t$的夹角，计算代价较大)或非精确(设置满足的Wolfe条件的步长)的线搜索方式来确定，这种方式叫非线性共轭梯度法。非线性共轭梯度法主要分为两种：在其更新步骤$\boldsymbol{d}_{t} = -\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}) + \beta_t \boldsymbol{d}_{t-1}$中，若$\beta_t$恒用$\beta_t = (\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}))^\intercal \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t})/[(\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_{t-1}}))^\intercal \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_{t-1}})]$的更新方式则称为FR方式，记为$\beta_t^{FR}$；若恒用$\beta_t = [(\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t})-(\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_{t-1}}))^\intercal \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t})]/[(\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_{t-1}}))^\intercal \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_{t-1}})]$则称为PRP方式，记为$\beta_t^{PRP}$。两种更新方式的收敛性在《数学规划》(黄选红，韩继业，2006)中有详细证明过程。注意到当$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}) \to \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_{t-1}})$且非常小时，$\beta_t^{FR} \to 1$而$\beta_t^{PRP} \to 0$，所以对于FR方式来说，在新的梯度方向上几乎很难更新，因此需要设置在每进行一定的步长后重启线性搜索(即$\beta_t^{FR} $置零)；而对PRP方式而言相当于当梯度足够小时，会自动重启线性搜索过程，因而它在数值计算方面表现较好。

以上都是在严格凸函数的前提下的结论，特别地，若$\boldsymbol{d}_{t} = -\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta_t}) + \beta_t^{PRP} \boldsymbol{d}_{t-1}$中将$\beta_t^{PRP}$换为$\beta_t^{PRP+}$使$\beta_t^{PRP+} = ReLU(\beta_t^{PRP})$，那么这个方法对非凸函数仍然适用。

##拟牛顿法

牛顿法最大优点是利用了函数的曲率信息(Hessian矩阵)使得收敛速度很快。但Hessian矩阵的计算开销太大，拟牛顿法的核心思想是利用一阶信息去构造一个与原Hessian矩阵近似的矩阵，并由此产生新的探索方向。

设代价函数在$\boldsymbol{\theta}^{t+1}$二阶可微，则对其在$\boldsymbol{\theta}^{t+1}$进行一阶泰勒展开：

$$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}) = \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{t+1}) + \nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}^{t+1})(\boldsymbol{\theta}-\boldsymbol{\theta}^{t+1})$$

将$\boldsymbol{\theta} = \boldsymbol{\theta}^t$代入上式后有：

$$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{t+1}) - \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta})^t = \nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}^{t+1})(\boldsymbol{\theta}^{t+1}-\boldsymbol{\theta}^t)$$

使$\nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{t+1}) - \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^t) = \boldsymbol{y}^t$且$\boldsymbol{\theta}^{t+1}-\boldsymbol{\theta}^t = \boldsymbol{s}^t$上式变成$\nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}^{t+1}) \boldsymbol{s}^t = \boldsymbol{y}^t$或$\boldsymbol{s}^t = \nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}^{t+1})^{-1} \boldsymbol{y}^t$。设$\boldsymbol{B}_{t+1}$与$\boldsymbol{H}_{t+1}$分别为$\nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}^{t+1})$和$\nabla_{\boldsymbol{\theta}}^2 J(\boldsymbol{\theta}^{t+1})^{-1}$的近似矩阵，则$\boldsymbol{B}_{t+1} \boldsymbol{s}^t = \boldsymbol{y}^t$和$\boldsymbol{s}^t = \boldsymbol{H}_{t+1} \boldsymbol{y}^t$称为拟牛顿条件。

牛顿法的线搜索过程$\boldsymbol{x}^{t+1} = \boldsymbol{x}^{t} + \alpha_t \boldsymbol{d}_t$与共轭梯度法中的方法一致，而梯度的更新规则为：$\boldsymbol{d}^t = -\boldsymbol{H}_t \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^t)$，在计算下一步的$\boldsymbol{H}_{t+1}$时，不直接进行计算，而是利用$\boldsymbol{g}^{t+1} = \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{t+1})$对矩阵进行校正(以满足拟牛顿条件)。

校正方式分为两类：对称秩一校正(SR1校正)和对称秩二校正(SR2校正)。

所谓秩一校正，就是在原矩阵上增加一个秩为1的新矩阵，通常为$\boldsymbol{uv}^\intercal$，$\boldsymbol{u}$和$\boldsymbol{v}^\intercal$可以任取使其满足拟牛顿条件：$\boldsymbol{H}_{t+1}\boldsymbol{y}^t = (\boldsymbol{H}_t + \boldsymbol{uv}^\intercal) \boldsymbol{y}^t = \boldsymbol{s}^t$，$\therefore (\boldsymbol{v}^\intercal \boldsymbol{y}^t) \boldsymbol{u} = \boldsymbol{s}^t - \boldsymbol{H}_t \boldsymbol{y}^t$。将$\boldsymbol{u}$代入$\boldsymbol{H}_{t+1} = \boldsymbol{H}_t + \boldsymbol{uv}^\intercal$中有：

$$\boldsymbol{H}_{t+1} = \boldsymbol{H}_t + \frac{(\boldsymbol{s}^t - \boldsymbol{H}_t \boldsymbol{y}^t)\boldsymbol{v}^\intercal}{\boldsymbol{v}^\intercal \boldsymbol{y}^t}$$

已知$\boldsymbol{H}_{t}$对称(严格凸函数前提)，为满足$\boldsymbol{H}_{t+1}^\intercal = \boldsymbol{H}_{t+1}$(对称性)，则可以使$\boldsymbol{v} = \boldsymbol{s}^t - \boldsymbol{H}_t \boldsymbol{y}^t$后有：

$$\boldsymbol{H}_{t+1} = \boldsymbol{H}_t + \frac{(\boldsymbol{s}^t - \boldsymbol{H}_t \boldsymbol{y}^t)\boldsymbol{(\boldsymbol{s}^t - \boldsymbol{H}_t \boldsymbol{y}^t)}^\intercal}{\boldsymbol{(\boldsymbol{s}^t - \boldsymbol{H}_t \boldsymbol{y}^t)}^\intercal \boldsymbol{y}^t}$$

用Sherman-Morrison定理可推出：

$$\boldsymbol{B}_{t+1} = \boldsymbol{B}_t + \frac{(\boldsymbol{y}^t - \boldsymbol{B}_t \boldsymbol{s}^t)\boldsymbol{(\boldsymbol{y}^t - \boldsymbol{B}_t \boldsymbol{s}^t)}^\intercal}{\boldsymbol{(\boldsymbol{y}^t - \boldsymbol{B}_t \boldsymbol{s}^t)}^\intercal \boldsymbol{s}^t}$$

关于SR2校正，李航的《统计学习方法》里的介绍比较容易理解。每次更新矩阵时，都是在原矩阵上附加两个待定矩阵构成：$\boldsymbol{G}_{t+1} = \boldsymbol{G}_t + \boldsymbol{P}_t + \boldsymbol{Q}_t$。

当待更新矩阵为$\boldsymbol{G}_{t+1} = \boldsymbol{H}_{t+1}$时，对应着DFP算法(应用的拟牛顿条件为$\boldsymbol{s}^t = \boldsymbol{H}_{t+1} \boldsymbol{y}^t$):

$$\boldsymbol{H}_{t+1} \boldsymbol{y}^t = \boldsymbol{H}_t \boldsymbol{y}^t + \boldsymbol{P}_t \boldsymbol{y}^t + \boldsymbol{Q}_t \boldsymbol{y}^t$$

使$\boldsymbol{P}_t \boldsymbol{y}^t = \boldsymbol{s}^t$以及$\boldsymbol{Q}_t \boldsymbol{y}^t = - \boldsymbol{H}_t \boldsymbol{y}^t$使拟牛顿条件成立，不难发现可以取：

$$\boldsymbol{P}_t = \frac{\boldsymbol{s}^t\boldsymbol{s}^{t\intercal}}{\boldsymbol{s}^{t\intercal}\boldsymbol{y}^t}; \quad \boldsymbol{Q}_t = -\frac{\boldsymbol{H}_t \boldsymbol{y}^t \boldsymbol{y}^{t\intercal}\boldsymbol{H}_t}{\boldsymbol{y}^{t\intercal}\boldsymbol{H}_t \boldsymbol{y}^t}$$

则$\boldsymbol{H}_{t+1}$校正公式为：

$$\boldsymbol{H}_{t+1}^{DFP} = \boldsymbol{H}_t - \frac{\boldsymbol{H}_t \boldsymbol{y}^t \boldsymbol{y}^{t\intercal}\boldsymbol{H}_t}{\boldsymbol{y}^{t\intercal}\boldsymbol{H}_t \boldsymbol{y}^t} + \frac{\boldsymbol{s}^t\boldsymbol{s}^{t\intercal}}{\boldsymbol{s}^{t\intercal}\boldsymbol{y}^t}$$

与之对应的$\boldsymbol{B}_{t+1}$用Sherman-Morrison定理可推出：

$$\boldsymbol{B}_{t+1}^{DFP} = (\boldsymbol{I} - \frac{\boldsymbol{y}^t\boldsymbol{s}^{t\intercal}}{\boldsymbol{y}^{t\intercal}\boldsymbol{s}^t}) \boldsymbol{B}_t (\boldsymbol{I} - \frac{\boldsymbol{y}^t\boldsymbol{s}^{t\intercal}}{\boldsymbol{y}^{t\intercal}\boldsymbol{s}^t})^\intercal + \frac{\boldsymbol{y}^t\boldsymbol{y}^{t\intercal}}{\boldsymbol{y}^{t\intercal}\boldsymbol{s}^t}$$

当待更新矩阵为$\boldsymbol{G}_{t+1} = \boldsymbol{B}_{t+1}$时，对应着BFGS算法(应用的拟牛顿条件为$\boldsymbol{B}_{t+1} \boldsymbol{s}^t = \boldsymbol{y}^t$)。类似上述求解过程可求得：

$$\boldsymbol{B}_{t+1}^{BFGS} = \boldsymbol{B}_t - \frac{\boldsymbol{B}_t \boldsymbol{s}^t \boldsymbol{s}^{t\intercal}\boldsymbol{B}_t}{\boldsymbol{s}^{t\intercal}\boldsymbol{B}_t \boldsymbol{s}^t} + \frac{\boldsymbol{y}^t\boldsymbol{y}^{t\intercal}}{\boldsymbol{y}^{t\intercal}\boldsymbol{s}^t}$$

$$\boldsymbol{H}_{t+1}^{BFGS} = (\boldsymbol{I} - \frac{\boldsymbol{s}^t\boldsymbol{y}^{t\intercal}}{\boldsymbol{s}^{t\intercal}\boldsymbol{y}^t}) \boldsymbol{H}_t (\boldsymbol{I} - \frac{\boldsymbol{s}^t\boldsymbol{y}^{t\intercal}}{\boldsymbol{s}^{t\intercal}\boldsymbol{y}^t})^\intercal + \frac{\boldsymbol{s}^t\boldsymbol{s}^{t\intercal}}{\boldsymbol{s}^{t\intercal}\boldsymbol{y}^t}$$

在数值优化中计算性能比较好的，最流行的拟牛顿方法是BFGS方法，但观察上式的递推关系，其计算复杂度为$O(t^2)$，不适用于深度学习的架构。从BFGS中改进的[L-BFGS](https://zhuanlan.zhihu.com/p/29672873)只保存最近m次的$\boldsymbol{s}^t$和$\boldsymbol{y}^t$，自动删除输早的项$\boldsymbol{s}^{t-m}$和$\boldsymbol{y}^{t-m}$并保持$\boldsymbol{s}^t = \boldsymbol{x}^{t+1} - \boldsymbol{x}^t$和$\boldsymbol{y}^t = \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^{t+1}) - \nabla_{\boldsymbol{\theta}}J(\boldsymbol{\theta}^t)$，从而使计算复杂度降到$O(t)$。