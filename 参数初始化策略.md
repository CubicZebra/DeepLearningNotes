#参数初始化策略

##权重的初始化

由Glorot提议使用的标准初始化方法的基本思想是，设第i-1层与第i层分别有$m_{i-1}$与$m_i$个单元，初始化使两层间权重满足均值为0，方差为$\frac{2}{m_{i-1}+m_i}$的分布，因而：

$$W_{i-1, i} \sim U(w_{i-1,i}; -\sqrt{\frac{6}{m_{i-1}+m_i}},\sqrt{\frac{6}{m_{i-1}+m_i}})\; or\; \mathcal{N}(w_{i-1,i}; 0, \sqrt{\frac{2}{m_{i-1}+m_i}})$$

以缩放准则作为权重的初始化策略是有缺点的，原文举的例子传达的意思是，层的规模本身也会影响到权重初始值的范数，不管是LeCun的初始化策略将权重初始为均值为0，方差为$\frac{1}{m}$的分布，还是Glorot的初始化策略将权重初始为均值为0，方差为$\frac{2}{m_{i-1}+m_i}$的分布，只要其分母m（层的规模，单元数）趋向于无穷，权重也将收敛于$\mathcal{N}(0,0)$。

关于Martens提出的稀疏初始化利弊的讨论结合之前病态Hessian条件（图8.5，图4.6），当固定选择每个单元恰好有k个非零权重时，一旦那些为零的权重更新了一个非常小的量（一个稀疏矩阵的突然更新），可以等效理解为此时的Hessian阵相较于初始时拥有了更大的条件数，结合图4.6不难发现，此时作随机梯度下降的话，路径很容易在这些新生成的具有很大特征值的维度上反复探索而偏离了最初始的优化问题（可以简单理解为运用这种策略后，程序一开始就会先制造一个比原问题更棘手的问题，然后SGD会在优化原问题的同时反复在制造出来的这部分困难上进行探索，因而等到去缩小那些理应被缩小的“大值”时实际已经耗费了过长的时间）。

##偏置的初始化

偏置的设置，第一点说明比较晦涩。举一个简单的例子，当输入为一个设计矩阵X，第一列为均值为166cm的身高，第二列为均值为47kg的体重，那么可以认为该分布是高度偏态的，如果未设置任何偏置，系统从0学习到166和47这两个数字将会是十分漫长的过程。从这个事例可以看出，如果一开始将偏置设置在(166,47)附近有助于系统学习到更符合事实的解(不一定是全局最优)，而从(0,0)到(166,47)的过程里很有可能陷入局部最优而得不到正确的结果。

对于这种问题，一个更简单的方法是将输入进行预处理，例如使所有边缘分布标准化为$\mathcal{N}(0,1)$分布，将这种输入在无偏置设置的结构里学习，和将原输入在设置合适的偏置中学习实际上是等效的。